{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buidling an Image Classifier Using Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55000, 28, 28)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we’ll scale the pixel intensities down to the 0–1 range by dividing them by 255.0 (this also converts them to floats):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With MNIST, when the label is equal to 5, it means that the image represents the handwritten digit 5. Easy. For Fashion MNIST, however, we need the list of class names to know what we are dealing with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "                   \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the model using the sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s build the neural network! Here is a classification MLP with two hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=[28,28]))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(300, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(100, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of adding the layers one by one as we just did, it’s often more convenient to pass a list of layers when creating the Sequential model. You can also drop the Input layer and instead specify the input_shape in the first layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
    "        tf.keras.layers.Dense(300, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s summary() method displays all the model’s layers,14 including each layer’s name (which is automatically generated unless you set it when creating the layer), its output shape (None means the batch size can be anything), and its number of parameters. The summary ends with the total number of parameters, including trainable and non-trainable parameters. Here we only have trainable parameters (you will see some non-trainable parameters later in this chapter):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten_3 (Flatten)         (None, 784)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 300)               235500    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 100)               30100     \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 266610 (1.02 MB)\n",
      "Trainable params: 266610 (1.02 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that Dense layers often have a lot of parameters. For example, the first hidden layer has 784 × 300 connection weights, plus 300 bias terms, which adds up to 235,500 parameters! This gives the model quite a lot of flexibility to fit the training data, but it also means that the model runs the risk of overfitting, especially when you do not have a lot of training data. We will come back to this later.\n",
    "\n",
    "Each layer in a model must have a unique name (e.g., \"dense_2\"). You can set the layer names explicitly using the constructor’s name argument, but generally it’s sim‐ pler to let Keras name the layers automatically, as we just did. Keras takes the layer’s class name and converts it to snake case (e.g., a layer from the MyCoolLayer class is named \"my_cool_layer\" by default). Keras also ensures that the name is globally unique, even across models, by appending an index if needed, as in \"dense_2\". But why does it bother making the names unique across models? Well, this makes it possible to merge models easily without getting name conflicts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily get a model’s list of layers using the layers attribute, or use the get_layer() method to access a layer by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.src.layers.reshaping.flatten.Flatten at 0x15665cb10>,\n",
       " <keras.src.layers.core.dense.Dense at 0x1565d5e10>,\n",
       " <keras.src.layers.core.dense.Dense at 0x15641be90>,\n",
       " <keras.src.layers.core.dense.Dense at 0x155c12fd0>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_19'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden1 = model.layers[1]\n",
    "hidden1.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer('dense_19') is hidden1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the parameters of a layer can be accessed using its get_weights() and set_weights() methods. For a Dense layer, this includes both the connection weights and the bias terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01171166, -0.0455946 , -0.06099164, ..., -0.01070148,\n",
       "        -0.06490641, -0.06298914],\n",
       "       [-0.01247824, -0.00236209,  0.07235271, ...,  0.0433299 ,\n",
       "        -0.02734338,  0.04987819],\n",
       "       [ 0.01917706, -0.02338834,  0.00536769, ..., -0.01429823,\n",
       "         0.06894374,  0.06259514],\n",
       "       ...,\n",
       "       [ 0.02124688, -0.01124633, -0.0061475 , ...,  0.00923929,\n",
       "        -0.0743577 , -0.036768  ],\n",
       "       [-0.01719093, -0.07415979,  0.04971988, ..., -0.01528203,\n",
       "        -0.01632241,  0.06814897],\n",
       "       [ 0.02364569,  0.06128085, -0.05449406, ..., -0.02653084,\n",
       "        -0.07317065, -0.06492712]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights, biases = hidden1.get_weights()\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a model is created, you must call its compile() method to specify the loss function and the optimizer to use. Optionally, you can specify a list of extra metrics to compute during training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using loss=\"sparse_categorical_crossentropy\" is the equivalent of using loss=tf.keras.losses.sparse_categorical_ crossentropy. Similarly, using optimizer=\"sgd\" is the equiva‐ lent of using optimizer=tf.keras.optimizers.SGD(), and using metrics=[\"accuracy\"] is the equivalent of using metrics= [tf.keras.metrics.sparse_categorical_accuracy] (when using this loss). We will use many other losses, optimizers, and metrics in this book; for the full lists, see https://keras.io/api/losses, https:// keras.io/api/optimizers, and https://keras.io/api/metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code requires explanation. We use the \"sparse_categorical_crossentropy\" loss because we have sparse labels (i.e., for each instance, there is just a target class index, from 0 to 9 in this case), and the classes are exclusive. If instead we had one target probability per class for each instance (such as one-hot vectors, e.g., [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.] to represent class 3), then we would need to use the \"categorical_crossentropy\" loss instead. If we were doing binary classification or multilabel binary classification, then we would use the \"sigmoid\" activation function in the output layer instead of the \"softmax\" activation function, and we would use the \"binary_crossentropy\" loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to convert sparse labels (i.e., class indices) to one-hot vector labels, use the tf.keras.utils.to_categorical() func‐ tion. To go the other way round, use the np.argmax() function with axis=1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding the optimizer, \"sgd\" means that we will train the model using stochastic gradient descent. In other words, Keras will perform the backpropagation algorithm described earlier (i.e., reverse-mode autodiff plus gradient descent).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the SGD optimizer, it is important to tune the learn‐ ing rate. So, you will generally want to use optimizer=tf.keras. optimizers.SGD(learning_rate=__???__) to set the learning rate, rather than optimizer=\"sgd\", which defaults to a learning rate of 0.01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since this is a classifier, it’s useful to measure its accuracy during training and evaluation, which is why we set metrics=[\"accuracy\"]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7298 - accuracy: 0.7600 - val_loss: 0.5065 - val_accuracy: 0.8304\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4896 - accuracy: 0.8303 - val_loss: 0.4591 - val_accuracy: 0.8342\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4423 - accuracy: 0.8453 - val_loss: 0.4238 - val_accuracy: 0.8528\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4180 - accuracy: 0.8545 - val_loss: 0.3966 - val_accuracy: 0.8604\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 2s 977us/step - loss: 0.3972 - accuracy: 0.8603 - val_loss: 0.3935 - val_accuracy: 0.8602\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 2s 979us/step - loss: 0.3814 - accuracy: 0.8661 - val_loss: 0.3965 - val_accuracy: 0.8604\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3691 - accuracy: 0.8693 - val_loss: 0.3741 - val_accuracy: 0.8680\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3576 - accuracy: 0.8736 - val_loss: 0.3773 - val_accuracy: 0.8604\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3472 - accuracy: 0.8774 - val_loss: 0.3518 - val_accuracy: 0.8706\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 2s 996us/step - loss: 0.3371 - accuracy: 0.8809 - val_loss: 0.3545 - val_accuracy: 0.8756\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3283 - accuracy: 0.8832 - val_loss: 0.3660 - val_accuracy: 0.8656\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.3204 - accuracy: 0.8854 - val_loss: 0.3492 - val_accuracy: 0.8712\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 2s 952us/step - loss: 0.3134 - accuracy: 0.8889 - val_loss: 0.3293 - val_accuracy: 0.8790\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 2s 994us/step - loss: 0.3059 - accuracy: 0.8902 - val_loss: 0.3423 - val_accuracy: 0.8784\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2996 - accuracy: 0.8927 - val_loss: 0.3341 - val_accuracy: 0.8822\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2924 - accuracy: 0.8947 - val_loss: 0.3345 - val_accuracy: 0.8762\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2875 - accuracy: 0.8964 - val_loss: 0.3379 - val_accuracy: 0.8760\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2815 - accuracy: 0.8987 - val_loss: 0.3246 - val_accuracy: 0.8804\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2761 - accuracy: 0.9001 - val_loss: 0.3462 - val_accuracy: 0.8708\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2713 - accuracy: 0.9023 - val_loss: 0.3233 - val_accuracy: 0.8792\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 2s 984us/step - loss: 0.2659 - accuracy: 0.9028 - val_loss: 0.3148 - val_accuracy: 0.8850\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2604 - accuracy: 0.9059 - val_loss: 0.3151 - val_accuracy: 0.8822\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2566 - accuracy: 0.9079 - val_loss: 0.3459 - val_accuracy: 0.8734\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 2s 991us/step - loss: 0.2513 - accuracy: 0.9090 - val_loss: 0.3206 - val_accuracy: 0.8814\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2474 - accuracy: 0.9109 - val_loss: 0.3128 - val_accuracy: 0.8882\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 2s 997us/step - loss: 0.2433 - accuracy: 0.9132 - val_loss: 0.3099 - val_accuracy: 0.8848\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2379 - accuracy: 0.9139 - val_loss: 0.3205 - val_accuracy: 0.8856\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 2s 956us/step - loss: 0.2354 - accuracy: 0.9164 - val_loss: 0.3065 - val_accuracy: 0.8884\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2310 - accuracy: 0.9166 - val_loss: 0.3096 - val_accuracy: 0.8882\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.2273 - accuracy: 0.9192 - val_loss: 0.3074 - val_accuracy: 0.8866\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pass it the input features (X_train) and the target classes (y_train), as well as the number of epochs to train (or else it would default to just 1, which would definitely not be enough to converge to a good solution). We also pass a validation set (this is optional). Keras will measure the loss and the extra metrics on this set at the end of each epoch, which is very useful to see how well the model really performs. If the performance on the training set is much better than on the validation set, your model is probably overfitting the training set, or there is a bug, such as a data mismatch between the training set and the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8,5), xlim=[0,29], ylim=[0,1], grid=True, xlabel='Epoch',\n",
    "    style=['r--', 'r--.', 'b-', 'b-*']\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3295632600784302, 0.8830999732017517]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the model to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.  , 0.  , 0.  , 0.01, 0.  , 0.02, 0.  , 0.98],\n",
       "       [0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each instance the model estimates one probability per class, from class 0 to class 9. This is similar to the output of the predict_proba() method in Scikit-Learn classifiers. For example, for the first image it estimates that the probability of class 9 (ankle boot) is 96%, the probability of class 7 (sneaker) is 2%, the probability of class 5 (sandal) is 1%, and the probabilities of the other classes are negligible. In other words, it is highly confident that the first image is footwear, most likely ankle boots but possibly sneakers or sandals. If you only care about the class with the highest estimated probability (even if that probability is quite low), then you can use the argmax() method to get the highest probability class index for each instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y_pred = y_proba.argmax(axis=-1)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new = y_test[:3]\n",
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Regression MLP Using Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s switch back to the California housing problem and tackle it using the same MLP as earlier, with 3 hidden layers composed of 50 neurons each, but this time building it with Keras.\n",
    "\n",
    "Using the sequential API to build, train, evaluate, and use a regression MLP is quite similar to what we did for classification. The main differences in the following code example are the fact that the output layer has a single neuron (since we only want to predict a single value) and it uses no activation function, the loss function is the mean squared error, the metric is the RMSE, and we’re using an Adam optimizer like Scikit-Learn’s MLPRegressor did. Moreover, in this example we don’t need a Flatten layer, and instead we’re using a Normalization layer as the first layer: it does the same thing as Scikit-Learn’s StandardScaler, but it must be fitted to the training data using its adapt() method before you call the model’s fit() method. Let’s take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.5526 - root_mean_squared_error: 2.9245 - val_loss: 8.4127 - val_root_mean_squared_error: 2.9006\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 8.3025 - root_mean_squared_error: 2.8814 - val_loss: 8.3916 - val_root_mean_squared_error: 2.8970\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.2867 - root_mean_squared_error: 2.8787 - val_loss: 8.4089 - val_root_mean_squared_error: 2.9000\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.2785 - root_mean_squared_error: 2.8772 - val_loss: 8.4471 - val_root_mean_squared_error: 2.9065\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 8.2698 - root_mean_squared_error: 2.8757 - val_loss: 8.3914 - val_root_mean_squared_error: 2.8970\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2596 - root_mean_squared_error: 2.8740 - val_loss: 8.3671 - val_root_mean_squared_error: 2.8928\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2599 - root_mean_squared_error: 2.8740 - val_loss: 8.3738 - val_root_mean_squared_error: 2.8940\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2541 - root_mean_squared_error: 2.8730 - val_loss: 8.3667 - val_root_mean_squared_error: 2.8927\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 8.2515 - root_mean_squared_error: 2.8725 - val_loss: 8.3659 - val_root_mean_squared_error: 2.8926\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.2501 - root_mean_squared_error: 2.8723 - val_loss: 8.3728 - val_root_mean_squared_error: 2.8937\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.2512 - root_mean_squared_error: 2.8725 - val_loss: 8.3701 - val_root_mean_squared_error: 2.8933\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2493 - root_mean_squared_error: 2.8722 - val_loss: 8.3982 - val_root_mean_squared_error: 2.8981\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 8.2494 - root_mean_squared_error: 2.8722 - val_loss: 8.4111 - val_root_mean_squared_error: 2.9003\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2478 - root_mean_squared_error: 2.8719 - val_loss: 8.3656 - val_root_mean_squared_error: 2.8925\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2483 - root_mean_squared_error: 2.8720 - val_loss: 8.3661 - val_root_mean_squared_error: 2.8926\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2464 - root_mean_squared_error: 2.8717 - val_loss: 8.3669 - val_root_mean_squared_error: 2.8928\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2457 - root_mean_squared_error: 2.8716 - val_loss: 8.3888 - val_root_mean_squared_error: 2.8965\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 8.2473 - root_mean_squared_error: 2.8718 - val_loss: 8.3658 - val_root_mean_squared_error: 2.8925\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 8.2463 - root_mean_squared_error: 2.8716 - val_loss: 8.3674 - val_root_mean_squared_error: 2.8928\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 3s 2ms/step - loss: 8.2474 - root_mean_squared_error: 2.8718 - val_loss: 8.3656 - val_root_mean_squared_error: 2.8925\n",
      "313/313 [==============================] - 0s 769us/step - loss: 8.2479 - root_mean_squared_error: 2.8719\n",
      "1/1 [==============================] - 0s 63ms/step\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)\n",
    "norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])\n",
    "model = tf.keras.Sequential([\n",
    "    norm_layer,\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['RootMeanSquaredError'])\n",
    "norm_layer.adapt(X_train)\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test, rmse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Normalization layer learns the feature means and standard deviations in the training data when you call the adapt() method. Yet when you display the model’s summary, these statistics are listed as non-trainable. This is because these parameters are not affected by gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the sequential API is quite clean and straightforward. However, although Sequential models are extremely common, it is sometimes useful to build neural networks with more complex topologies, or with multiple inputs or outputs. For this purpose, Keras offers the functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Complex Models Using the Functional API\n",
    "## NOT FINISHED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One example of a nonsequential neural network is a Wide & Deep neural network. This neural network architecture was introduced in a 2016 paper by Heng-Tze Cheng et al. It connects all or part of the inputs directly to the output layer. This architecture makes it possible for the neural network to learn both deep patterns (using the deep path) and simple rules (through the short path). In contrast, a regular MLP forces all the data to flow through the full stack of layers; thus, simple patterns in the data may end up being distorted by this sequence of transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s build such a neural network to tackle the California housing problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization_layer = tf.keras.layers.Normalization()\n",
    "hidden_layer1 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "hidden_layer2 = tf.keras.layers.Dense(30, activation=\"relu\")\n",
    "concat_layer = tf.keras.layers.Concatenate()\n",
    "output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "normalized = normalization_layer(input_)\n",
    "hidden1 = hidden_layer1(normalized)\n",
    "hidden2 = hidden_layer2(hidden1)\n",
    "concat = concat_layer([normalized, hidden2])\n",
    "output = output_layer(concat)\n",
    "\n",
    "model = tf.keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have built this Keras model, everything is exactly like earlier, so there’s no need to repeat it here: you compile the model, adapt the Normalization layer, fit the model, evaluate it, and use it to make predictions.\n",
    "\n",
    "But what if you want to send a subset of the features through the wide path and a different subset (possibly overlapping) through the deep path. In this case, one solution is to use multiple inputs. For example, suppose we want to send five features through the wide path (features 0 to 4), and six features through the deep path (features 2 to 7). We can do this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_wide = tf.keras.layers.Input(shape=[5]) # features 0 to 4 \n",
    "input_deep = tf.keras.layers.Input(shape=[6]) # features 2 to 7 \n",
    "norm_layer_wide = tf.keras.layers.Normalization() \n",
    "norm_layer_deep = tf.keras.layers.Normalization()\n",
    "norm_wide = norm_layer_wide(input_wide)\n",
    "norm_deep = norm_layer_deep(input_deep)\n",
    "hidden1 = tf.keras.layers.Dense(30, activation=\"relu\")(norm_deep)\n",
    "hidden2 = tf.keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = tf.keras.layers.concatenate([norm_wide, hidden2])\n",
    "output = tf.keras.layers.Dense(1)(concat)\n",
    "model = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compile the model as usual, but when we call the fit() method, instead of passing a single input matrix X_train, we must pass a pair of matrices (X_train_wide, X_train_deep), one per input. The same is true for X_valid, and also for X_test and X_new when you call evaluate() or predict():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=1e-3)\n",
    "model.compile(loss='mse', optimizer=optimizer, metrics=['RootMeanSquaredError'])\n",
    "\n",
    "X_train_wide, X_train_deep = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_wide, X_valid_deep = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_wide, X_test_deep = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_wide, X_new_deep = X_test_wide[:3], X_test_deep[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/robbieardison/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/robbieardison/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/layers/preprocessing/normalization.py\", line 319, in update_state  **\n        self.adapt_mean * existing_weight + batch_mean * batch_weight\n\n    ValueError: Dimensions must be equal, but are 6 and 4 for '{{node add_1}} = AddV2[T=DT_FLOAT](mul, mul_1)' with input shapes: [6], [4].\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m norm_layer_wide\u001b[38;5;241m.\u001b[39madapt(X_train_wide)\n\u001b[0;32m----> 2\u001b[0m norm_layer_deep\u001b[38;5;241m.\u001b[39madapt(X_train_deep)\n\u001b[1;32m      3\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit((X_train_wide, X_train_deep), y_train, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[1;32m      4\u001b[0m                     validation_data\u001b[38;5;241m=\u001b[39m((X_valid_wide, X_valid_deep), y_valid))\n\u001b[1;32m      5\u001b[0m mse_test \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate((X_test_wide, X_test_deep), y_test)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/layers/preprocessing/normalization.py:287\u001b[0m, in \u001b[0;36mNormalization.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madapt\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    242\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the mean and variance of values in a dataset.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \n\u001b[1;32m    244\u001b[0m \u001b[38;5;124;03m    Calling `adapt()` on a `Normalization` layer is an alternative to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;03m          argument is not supported with array inputs.\u001b[39;00m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 287\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39madapt(data, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, steps\u001b[38;5;241m=\u001b[39msteps)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/engine/base_preprocessing_layer.py:258\u001b[0m, in \u001b[0;36mPreprocessingLayer.adapt\u001b[0;34m(self, data, batch_size, steps)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m--> 258\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapt_function(iterator)\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m    260\u001b[0m             context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/th/55hfqgh912325b89wjzp3d_w0000gn/T/__autograph_generated_fileg3xa_1me.py:11\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__adapt_step\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m      9\u001b[0m data \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mnext\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(iterator),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     10\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m_adapt_maybe_build, (ag__\u001b[38;5;241m.\u001b[39mld(data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 11\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mupdate_state, (ag__\u001b[38;5;241m.\u001b[39mld(data),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/layers/preprocessing/normalization.py:319\u001b[0m, in \u001b[0;36mNormalization.update_state\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    313\u001b[0m batch_weight \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(batch_count, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype) \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\n\u001b[1;32m    314\u001b[0m     total_count, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_dtype\n\u001b[1;32m    315\u001b[0m )\n\u001b[1;32m    316\u001b[0m existing_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m batch_weight\n\u001b[1;32m    318\u001b[0m total_mean \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_mean \u001b[38;5;241m*\u001b[39m existing_weight \u001b[38;5;241m+\u001b[39m batch_mean \u001b[38;5;241m*\u001b[39m batch_weight\n\u001b[1;32m    320\u001b[0m )\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# The variance is computed using the lack-of-fit sum of squares\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# formula (see\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;66;03m# https://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares).\u001b[39;00m\n\u001b[1;32m    324\u001b[0m total_variance \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_variance \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madapt_mean \u001b[38;5;241m-\u001b[39m total_mean) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    326\u001b[0m ) \u001b[38;5;241m*\u001b[39m existing_weight \u001b[38;5;241m+\u001b[39m (\n\u001b[1;32m    327\u001b[0m     batch_variance \u001b[38;5;241m+\u001b[39m (batch_mean \u001b[38;5;241m-\u001b[39m total_mean) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    328\u001b[0m ) \u001b[38;5;241m*\u001b[39m batch_weight\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/robbieardison/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/engine/base_preprocessing_layer.py\", line 123, in adapt_step  *\n        self.update_state(data)\n    File \"/Users/robbieardison/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/layers/preprocessing/normalization.py\", line 319, in update_state  **\n        self.adapt_mean * existing_weight + batch_mean * batch_weight\n\n    ValueError: Dimensions must be equal, but are 6 and 4 for '{{node add_1}} = AddV2[T=DT_FLOAT](mul, mul_1)' with input shapes: [6], [4].\n"
     ]
    }
   ],
   "source": [
    "norm_layer_wide.adapt(X_train_wide)\n",
    "norm_layer_deep.adapt(X_train_deep)\n",
    "history = model.fit((X_train_wide, X_train_deep), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_wide, X_valid_deep), y_valid))\n",
    "mse_test = model.evaluate((X_test_wide, X_test_deep), y_test)\n",
    "y_pred = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Subclassing API to Build Dynamic Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the sequential API and the functional API are declarative: you start by declaring which layers you want to use and how they should be connected, and only then can you start feeding the model some data for training or inference. This has many advantages: the model can easily be saved, cloned, and shared; its structure can be displayed and analyzed; the framework can infer shapes and check types, so errors can be caught early (i.e., before any data ever goes through the model). It’s also fairly straightforward to debug, since the whole model is a static graph of layers. But the flip side is just that: it’s static. Some models involve loops, varying shapes, conditional branching, and other dynamic behaviors. For such cases, or simply if you prefer a more imperative programming style, the subclassing API is for you.\n",
    "\n",
    "With this approach, you subclass the Model class, create the layers you need in the constructor, and use them to perform the computations you want in the call() method. For example, creating an instance of the following WideAndDeepModel class gives us an equivalent model to the one we just built with the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(tf.keras.Model):\n",
    "    def __init__(self, units=30, activation=\"relu\", **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
    "        self.norm_layer_wide = tf.keras.layers.Normalization()\n",
    "        self.hidden1 = tf.keras.layers.Dense(units, activation=activation) \n",
    "        self.hidden2 = tf.keras.layers.Dense(units, activation=activation) \n",
    "        self.main_output = tf.keras.layers.Dense(1)\n",
    "        self.aux_output = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        input_wide, input_deep = inputs\n",
    "        norm_wide = self.norm_layer_wide(input_wide)\n",
    "        norm_deep = self.norm_layer_deep(input_deep)\n",
    "        hidden1 = self.hidden1(norm_deep)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = tf.keras.layers.concatenate([norm_wide, hidden2]) \n",
    "        output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return output, aux_output\n",
    "\n",
    "model = WideAndDeepModel(30, activation=\"relu\", name=\"my_cool_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example looks like the previous one, except we separate the creation of the layers17 in the constructor from their usage in the call() method. And we don’t need to create the Input objects: we can use the input argument to the call() method.\n",
    "\n",
    "Now that we have a model instance, we can compile it, adapt its normal‐ ization layers (e.g., using model.norm_layer_wide.adapt(...) and model.norm_ layer_deep.adapt(...)), fit it, evaluate it, and use it to make predictions, exactly like we did with the functional API.\n",
    "\n",
    "The big difference with this API is that you can include pretty much anything you want in the call() method: for loops, if statements, low-level TensorFlow operations—your imagination is the limit. This makes it a great API when experimenting with new ideas, especially for researchers. However, this extra flexibility does come at a cost: your model’s architecture is hidden within the call() method, so Keras cannot easily inspect it; the model cannot be cloned using tf.keras.models.clone_model(); and when you call the summary() method, you only get a list of layers, without any information on how they are connected to each other. Moreover, Keras cannot check types and shapes ahead of time, and it is easier to make mistakes. So unless you really need that extra flexibility, you should probably stick to the sequential API or the functional API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving and Restoring a Model\n",
    "Saving a trained Keras model is as simple as it gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.WideAndDeepModel object at 0x2e08d1950>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.WideAndDeepModel object at 0x2e08d1950>, because it is not built.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model <__main__.WideAndDeepModel object at 0x2e08d1950> cannot be saved either because the input shape is not available or because the forward pass of the model is not defined.To define a forward pass, please override `Model.call()`. To specify an input shape, either call `build(input_shape)` directly, or call the model on actual data using `Model()`, `Model.fit()`, or `Model.predict()`. If you have a custom training step, please make sure to invoke the forward pass in train step through `Model.__call__`, i.e. `model(inputs)`, as opposed to `model.call()`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_keras_model\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/saving/legacy/saving_utils.py:97\u001b[0m, in \u001b[0;36mraise_model_input_error\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be saved because the input shape is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable. Please specify an input shape either by calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`build(input_shape)` directly, or by calling the model on actual \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata using `Model()`, `Model.fit()`, or `Model.predict()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# If the model is not a `Sequential`, it is intended to be a subclassed\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot be saved either because the input shape is not \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable or because the forward pass of the model is not defined.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo define a forward pass, please override `Model.call()`. To specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man input shape, either call `build(input_shape)` directly, or call \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe model on actual data using `Model()`, `Model.fit()`, or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.predict()`. If you have a custom training step, please make \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msure to invoke the forward pass in train step through \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`Model.__call__`, i.e. `model(inputs)`, as opposed to `model.call()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    106\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Model <__main__.WideAndDeepModel object at 0x2e08d1950> cannot be saved either because the input shape is not available or because the forward pass of the model is not defined.To define a forward pass, please override `Model.call()`. To specify an input shape, either call `build(input_shape)` directly, or call the model on actual data using `Model()`, `Model.fit()`, or `Model.predict()`. If you have a custom training step, please make sure to invoke the forward pass in train step through `Model.__call__`, i.e. `model(inputs)`, as opposed to `model.call()`."
     ]
    }
   ],
   "source": [
    "model.save(\"my_keras_model\", save_format=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you set save_format=\"tf\",18 Keras saves the model using TensorFlow’s Saved‐ Model format: this is a directory (with the given name) containing several files and subdirectories. In particular, the saved_model.pb file contains the model’s architecture and logic in the form of a serialized computation graph, so you don’t need to deploy the model’s source code in order to use it in production; the SavedModel is sufficient. The keras_metadata.pb file contains extra information needed by Keras. The variables subdirectory contains all the parameter values (including the connection weights, the biases, the normalization statistics, and the optimizer’s parameters), possibly split across multiple files if the model is very large. Lastly, the assets directory may contain extra files, such as data samples, feature names, class names, and so on. By default, the assets directory is empty. Since the optimizer is also saved, including its hyperparameters and any state it may have, after loading the model you can continue training if you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will typically have a script that trains a model and saves it, and one or more scripts (or web services) that load the model and use it to evaluate it or to make predictions. Loading the model is just as easy as saving it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at my_keras_model",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_keras_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m y_pred_main, y_pred_aux \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict((X_new_wide, X_new_deep))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/saving/saving_api.py:238\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    231\u001b[0m         filepath,\n\u001b[1;32m    232\u001b[0m         custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects,\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m,\n\u001b[1;32m    234\u001b[0m         safe_mode\u001b[38;5;241m=\u001b[39msafe_mode,\n\u001b[1;32m    235\u001b[0m     )\n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Legacy case.\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m legacy_sm_saving_lib\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[1;32m    239\u001b[0m     filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    240\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-pro/lib/python3.11/site-packages/keras/src/saving/legacy/save.py:234\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 234\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[1;32m    235\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    236\u001b[0m         )\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    239\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    240\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[1;32m    241\u001b[0m         )\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at my_keras_model"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"my_keras_model\")\n",
    "y_pred_main, y_pred_aux = model.predict((X_new_wide, X_new_deep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use save_weights() and load_weights() to save and load only the parameter values. This includes the connection weights, biases, preprocessing stats, optimizer state, etc. The parameter values are saved in one or more files such as my_weights.data-00004-of-00052, plus an index file like my_weights.index.\n",
    "\n",
    "Saving just the weights is faster and uses less disk space than saving the whole model, so it’s perfect to save quick checkpoints during training. If you’re training a big model, and it takes hours or days, then you must save checkpoints regularly in case the computer crashes. But how can you tell the fit() method to save checkpoints? Use callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The fit() method accepts a callbacks argument that lets you specify a list of objects that Keras will call before and after training, before and after each epoch, and even before and after processing each batch. For example, the ModelCheckpoint callback saves checkpoints of your model at regular intervals during training, by default at the end of each epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_checkpoints\", \n",
    "                                                   save_weights_only=True)\n",
    "\n",
    "history = model.fit([...], callbacks=[checkpoint_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, if you use a validation set during training, you can set save_ best_only=True when creating the ModelCheckpoint. In this case, it will only save your model when its performance on the validation set is the best so far. This way, you do not need to worry about training for too long and overfitting the training set: simply restore the last saved model after training, and this will be the best model on the validation set. This is one way to implement early stopping, but it won’t actually stop training.\n",
    "\n",
    "Another way is to use the EarlyStopping callback. It will interrupt training when it measures no progress on the validation set for a number of epochs (defined by the patience argument), and if you set restore_best_weights=True it will roll back to the best model at the end of training. You can combine both callbacks to save checkpoints of your model in case your computer crashes, and interrupt training early when there is no more progress, to avoid wasting time and resources and to reduce overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10, \n",
    "                                                     restore_best_weights=True)\n",
    "history = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of epochs can be set to a large value since training will stop automati‐ cally when there is no more progress (just make sure the learning rate is not too small, or else it might keep making slow progress until the end). The EarlyStopping callback will store the weights of the best model in RAM, and it will restore them for you at the end of training.\n",
    "\n",
    "*Many other callbacks are available in the tf.keras.callbacks package.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need extra control, you can easily write your own custom callbacks. For example, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrintValTrainRatioCallback(tf.keras.callbacks.Callback): \n",
    "      def on_epoch_end(self, epoch, logs):\n",
    "            ratio = logs[\"val_loss\"] / logs[\"loss\"]\n",
    "            print(f\"Epoch={epoch}, val/train={ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Call‐ backs can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end(), which are called by evaluate(). For prediction, you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end(), which are called by predict().\n",
    "\n",
    "Now let’s take a look at one more tool you should definitely have in your toolbox when using Keras: TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TensorBoard for Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-pro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
